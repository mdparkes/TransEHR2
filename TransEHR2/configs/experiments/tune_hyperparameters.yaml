EXPERIMENT_NAME: "tune_hyperparameters"

# List of hyperparameter names for easy access during tuning
# These hyperparameters' values must be lists of options to try
HYPERPARAMETERS_TO_TUNE:
  - DISC_LOSS_WEIGHT
  - THP_LOSS_NLL_WEIGHT
  - THP_PRED_LOSS_TIME_WT
  - RECORD_MASK_RATIO
  - CMPNT_MASK_RATIO
  - PRETRAIN_LEARNING_RATE
  - PRETRAIN_LEARNING_RATE_DECAY

# Values to test
DISC_LOSS_WEIGHT:
  - 1.0
  - 10.0
  - 100.0
THP_LOSS_NLL_WEIGHT:
  - 1.0
  - 0.001
  - 0.0001
THP_PRED_LOSS_TIME_WT:
  - 1.0
  - 0.001
  - 0.0001
RECORD_MASK_RATIO:
  - 0.15
  - 0.25
  - 0.50
CMPNT_MASK_RATIO:
  - 0.25
  - 0.5
  - 0.75
PRETRAIN_LEARNING_RATE:
  - 0.1
  - 0.01
  - 0.001
PRETRAIN_LEARNING_RATE_DECAY: 
  - 0.90
  - 0.80
  - 0.50

BATCH_SIZE: 200

# Whether to use text data as features
USE_TEXT: True

# Whether to let the generator and discriminator predict feature presence indicators as well as values
PREDICT_INDICATORS: False

GENERATOR_ENCODER_D_MODEL: 256
GENERATOR_ENCODER_N_HEADS: 2
GENERATOR_ENCODER_N_ENCODER_BLOCKS: 1
GENERATOR_ENCODER_DIM_FEEDFORWARD: 256
GENERATOR_ENCODER_DROPOUT: 0.1
GENERATOR_ENCODER_ACTIVATION: 'gelu'
GENERATOR_ENCODER_NORM: 'LayerNorm'

DISCRIMINATOR_ENCODER_D_MODEL: 256
DISCRIMINATOR_ENCODER_N_HEADS: 2
DISCRIMINATOR_ENCODER_N_ENCODER_BLOCKS: 1
DISCRIMINATOR_ENCODER_DIM_FEEDFORWARD: 256
DISCRIMINATOR_ENCODER_DROPOUT: 0.1
DISCRIMINATOR_ENCODER_ACTIVATION: 'gelu'
DISCRIMINATOR_ENCODER_NORM: 'LayerNorm'

THP_ENCODER_D_MODEL: 256
THP_ENCODER_D_INNER: 256
THP_ENCODER_N_LAYERS: 1
THP_ENCODER_N_HEADS: 2
THP_ENCODER_D_K: 64
THP_ENCODER_D_V: 64
THP_ENCODER_DROPOUT: 0.1

GENERATOR_D_MODEL: 256
GENERATOR_DIM_FEEDFORWARD: 256

DISCRIMINATOR_DIM_FEEDFORWARD: 256

PREDICTOR_AGGREGATION_METHOD: "mean"

MODEL_DIR: "/uhome/pr3/projects/p60290_2/TransEHR2/models"

PRETRAIN_TOTAL_EPOCH: 2000

THP_LOSS_MC_SAMPLES: 100
USE_THP_PRED_LOSS: True
THP_PRED_LOSS_TYPE_WT: 1.0

OBS_UNOBS_SAMPLE_RATIO: 4.0

# Finetuning parameters are not used during hyperparameter tuning, but set here for completeness
FINETUNE_LEARNING_RATE: 0.001
FINETUNE_TOTAL_EPOCH: 500
FINETUNE_LEARNING_RATE_DECAY: 0.80
